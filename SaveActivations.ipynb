{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cPickle\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import scipy.misc\n",
    "from collections import OrderedDict\n",
    "\n",
    "import utils\n",
    "#SGD_BATCHSIZE = 128\n",
    "SGD_BATCHSIZE = 128\n",
    "SGD_LEARNINGRATE = 0.001\n",
    "\n",
    "if True:\n",
    "    #LAYER_DIMS = [1024, 20, 20, 20]\n",
    "    LAYER_DIMS = [32, 28, 24, 20, 16, 12, 8, 8]\n",
    "    #LAYER_DIMS = [100, 50, 20]\n",
    "    #LAYER_DIMS = [50, 20, 20, 20] # 0.967 w. 128\n",
    "    #LAYER_DIMS = [20, 20, 20, 20] # 0.967 w. 128\n",
    "    #LAYER_DIMS = [128, 64, 32, 16, 16] # 0.967 w. 128\n",
    "    #LAYER_DIMS = [20, 20, 20, 20, 20, 20] # 0.967 w. 128\n",
    "    ARCH_NAME =  '-'.join(map(str,LAYER_DIMS))\n",
    "    trn, tst = utils.get_mnist()\n",
    "else:\n",
    "    ARCH_NAME = 'conv1'\n",
    "    trn, tst = utils.get_mnist2d()\n",
    "\n",
    "#ACTIVATION = 'relu'\n",
    "ACTIVATION = 'tanh'\n",
    "\n",
    "#SGD_BATCHSIZE = 512\n",
    "NUM_EPOCHS     = 10000\n",
    "NUMBER_OF_BINS = 5\n",
    "\n",
    "\n",
    "\n",
    "BASE_DIR = 'rawdata/' + ACTIVATION + '_' + ARCH_NAME\n",
    "\n",
    "\n",
    "import os\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    os.makedirs(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Reporter(keras.callbacks.Callback):\n",
    "    def __init__(self, whentodo=None, *kargs, **kwargs):\n",
    "        super(Reporter, self).__init__(*kargs, **kwargs)\n",
    "        if whentodo is None:\n",
    "            whentodo = lambda epoch: True\n",
    "        self.whentodo = whentodo\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.layerfuncs = []\n",
    "        self.layerweights = []\n",
    "        self.layerixs = []\n",
    "        for lndx, l in enumerate(self.model.layers):\n",
    "            if hasattr(l, 'kernel'):\n",
    "                self.layerixs.append(lndx)\n",
    "                self.layerfuncs.append(K.function(self.model.inputs, [l.output,]))\n",
    "                self.layerweights.append(l.kernel)\n",
    "            \n",
    "        input_tensors = [model.inputs[0],\n",
    "                         model.sample_weights[0],\n",
    "                         model.targets[0],\n",
    "                         K.learning_phase(),\n",
    "        ]\n",
    "        self.get_gradients = K.function(inputs=input_tensors, outputs=model.optimizer.get_gradients(model.total_loss, self.layerweights))\n",
    "        self.get_loss = K.function(inputs=input_tensors, outputs=model.total_loss)\n",
    "            \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if not self.whentodo(epoch):\n",
    "            self._log_gradients = False\n",
    "        else:\n",
    "            self._log_gradients = True\n",
    "            self._batch_weightnorm = []\n",
    "                \n",
    "            self._batch_gradients = [ [] for _ in self.model.layers[1:] ]\n",
    "            ixs = list(range(len(trn.X)))\n",
    "            np.random.shuffle(ixs)\n",
    "            self._batch_todo_ixs = ixs\n",
    "\n",
    "            \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        if not self._log_gradients:\n",
    "            return\n",
    "        \n",
    "        cur_ixs = self._batch_todo_ixs[:SGD_BATCHSIZE]\n",
    "        inputs = [trn.X[cur_ixs,:], [1,]*len(cur_ixs), trn.Y[cur_ixs,:], 1]\n",
    "        for lndx, g in enumerate(self.get_gradients(inputs)):\n",
    "            # g is gradients for weights of lndx's layer\n",
    "            oneDgrad = np.reshape(g, -1, 1)\n",
    "            self._batch_gradients[lndx].append(oneDgrad)\n",
    "\n",
    "        # Advance the indexing\n",
    "        self._batch_todo_ixs = self._batch_todo_ixs[SGD_BATCHSIZE:]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if not self.whentodo(epoch):\n",
    "            return\n",
    "        \n",
    "        self._log_gradients = True\n",
    "        \n",
    "        # Get overall performance\n",
    "        loss = {}\n",
    "        for cdata, cdataname, istrain in ((trn,'trn',1), (tst, 'tst',0)):\n",
    "            loss[cdataname] = self.get_loss([cdata.X, [1,]*len(cdata.X), cdata.Y, istrain]).flat[0]\n",
    "            \n",
    "        # Based on https://github.com/ravidziv/IDNNs/blob/1c4926f641d4306af7ae37325358be19e8f4d276/idnns/plots/plot_gradients.py\n",
    "        \n",
    "        data = {\n",
    "            'weights_norm':[],\n",
    "            'gradmean': [],\n",
    "            'gradstd': [],\n",
    "            'activity_tst': []\n",
    "        }\n",
    "        \n",
    "        for lndx, layerix in enumerate(self.layerixs):\n",
    "            clayer = self.model.layers[layerix]\n",
    "            weights_norm = np.linalg.norm(K.get_value(clayer.kernel))\n",
    "            stackedgrads = np.stack(self._batch_gradients[lndx], axis=1)\n",
    "            gradmean = np.linalg.norm(stackedgrads.mean(axis=1))\n",
    "            gradstd  = np.linalg.norm(stackedgrads.std(axis=1))\n",
    "            \n",
    "            data['weights_norm'].append(weights_norm)\n",
    "            data['gradmean'].append(gradmean)\n",
    "            data['gradstd'].append(gradstd)\n",
    "            \n",
    "            #trndata = trn.X[::20]\n",
    "            #tstdata = tst.X[::10]\n",
    "            #data['activity_trn'].append(self.layerfuncs[lndx]([trn.X,])[0])\n",
    "            data['activity_tst'].append(self.layerfuncs[lndx]([tst.X,])[0])\n",
    "            \n",
    "        fname = BASE_DIR + \"/epoch%08d\"% epoch\n",
    "        print(\"Saving\", fname)\n",
    "        with open(fname, 'wb') as f:\n",
    "             cPickle.dump({'ACTIVATION':ACTIVATION, 'epoch':epoch, 'data':data, 'loss':loss}, f, cPickle.HIGHEST_PROTOCOL)        \n",
    "        \n",
    "            \n",
    "if ARCH_NAME == 'conv1':\n",
    "    input_layer  = keras.layers.Input((trn.X.shape[1],trn.X.shape[2],1))\n",
    "    clayer = keras.layers.Conv2D(32, kernel_size=(3, 3), activation=ACTIVATION)(input_layer)\n",
    "    clayer = keras.layers.Conv2D(32, kernel_size=(3, 3), activation=ACTIVATION)(clayer)\n",
    "    clayer = keras.layers.MaxPooling2D(pool_size=(2,2))(clayer)\n",
    "    clayer = keras.layers.Flatten()(clayer)\n",
    "    clayer = keras.layers.Dense(20, activation=ACTIVATION)(clayer)\n",
    "    clayer = keras.layers.Dense(20, activation=ACTIVATION)(clayer)\n",
    "else:\n",
    "    input_layer  = keras.layers.Input((trn.X.shape[1],))\n",
    "    clayer = input_layer\n",
    "    for n in LAYER_DIMS:\n",
    "        clayer = keras.layers.Dense(n, activation=ACTIVATION)(clayer)\n",
    "\n",
    "outputs  = keras.layers.Dense(trn.nb_classes, activation='softmax')(clayer)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=outputs)\n",
    "optimizer = keras.optimizers.SGD(lr=SGD_LEARNINGRATE)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "def do_report(epoch):\n",
    "    if epoch < 20:\n",
    "        return True\n",
    "    elif epoch < 100:\n",
    "        return (epoch % 5 == 0)\n",
    "    elif epoch < 200:\n",
    "        return (epoch % 10 == 0)\n",
    "    else:\n",
    "        return (epoch % 100 == 0)\n",
    "    \n",
    "reporter   = Reporter(whentodo=do_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#r = model.fit(x=trn.X, y=trn.Y, verbose=2, batch_size=SGD_BATCHSIZE, epochs=NUM_EPOCHS, \n",
    "#              validation_data=(tst.X, tst.Y), callbacks=[reporter,])\n",
    "r = model.fit(x=trn.X, y=trn.Y, verbose=2, batch_size=SGD_BATCHSIZE, epochs=NUM_EPOCHS, callbacks=[reporter,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "asdadsf\n",
    "epochs = sorted(reporter.saved_logs.keys())\n",
    "cm = plt.cm.get_cmap('inferno')\n",
    "\n",
    "for colndx, t in enumerate(['trn','tst']):\n",
    "    loss = [reporter.saved_logs[epoch][t+'_loss'] for epoch in epochs]\n",
    "    plt.figure()\n",
    "    for lndx, layerid in enumerate(PLOT_LAYERS):\n",
    "        upperh = [reporter.saved_logs[epoch][t+'_layer_%d_h_upper' % lndx] for epoch in epochs]\n",
    "        lowerh = [reporter.saved_logs[epoch][t+'_layer_%d_h_lower' % lndx] for epoch in epochs]\n",
    "        sc=plt.scatter(loss, upperh, c=epochs, cmap=cm, edgecolor='none', label=\"$H_{KL}$\")\n",
    "        plt.scatter(loss, lowerh, c=epochs, cmap=cm, edgecolor='none', label=\"$H_{BD}$\")\n",
    "    plt.colorbar(sc, label='Epoch')\n",
    "    plt.xlabel('Cross-entropy loss')\n",
    "    plt.ylabel('H(hidden layer))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sortedk = sorted(reporter.saved_batch_logs.keys())\n",
    "meangrads = np.array([reporter.saved_batch_logs[k][0] for k in sortedk])\n",
    "stdgrads  = np.array([reporter.saved_batch_logs[k][1] for k in sortedk])\n",
    "\n",
    "plt.plot(sortedk, meangrads, label='m')\n",
    "plt.hold('on')\n",
    "plt.plot(sortedk, stdgrads, label='std')\n",
    "\n",
    "\n",
    "asdfassdfsfd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
