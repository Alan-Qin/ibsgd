{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import scipy.misc\n",
    "from collections import OrderedDict\n",
    "\n",
    "import utils\n",
    "trn, tst = utils.get_mnist()\n",
    "PLOT_LAYERS    = [2,3]\n",
    "NUM_EPOCHS     = 10000\n",
    "NUMBER_OF_BINS = 3\n",
    "\n",
    "SGD_BATCHSIZE = 128\n",
    "\n",
    "#ACTIVATION = 'relu'\n",
    "ACTIVATION = 'tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import simplebinmi, kde\n",
    "\n",
    "def wrapKfunc(f, data):\n",
    "    def callf(logvar):\n",
    "        return f([data, 1, np.exp(logvar)])[0].flat[0]\n",
    "    return callf\n",
    "\n",
    "class Reporter(keras.callbacks.Callback):\n",
    "    def __init__(self, whentodo=None, *kargs, **kwargs):\n",
    "        super(Reporter, self).__init__(*kargs, **kwargs)\n",
    "        if whentodo is None:\n",
    "            whentodo = lambda epoch: True\n",
    "        \n",
    "        self.whentodo = whentodo\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.layerfuncs = []\n",
    "        self.layerweights = []\n",
    "        \n",
    "        var = K.placeholder(ndim=0)\n",
    "        inputs = self.model.inputs + [ K.learning_phase(),] + [var,]\n",
    "        for lndx in PLOT_LAYERS:\n",
    "            l = self.model.layers[lndx]\n",
    "            cfuncs = {}\n",
    "            cfuncs['loo']    = K.function(inputs, [kde.kde_entropy_from_dists_loo(kde.Kget_dists(l.output),l.output,var)])\n",
    "            cfuncs['upper']  = K.function(inputs, [kde.entropy_estimator(l.output,var)])\n",
    "            #cfuncs['lower']  = K.function(inputs, [entropy_estimator(l.output,var, 0.25)])\n",
    "            cfuncs['lower'] = K.function(inputs, [kde.entropy_estimator(l.output,4*var)+np.log(0.25)*l.output.shape[1]/2.])\n",
    "            #cfuncs['lower']  = K.function(inputs, [entropy_estimator(l.output,var, 0.25)])\n",
    "            cfuncs['output'] = K.function(self.model.inputs + [ K.learning_phase(),], [l.output])\n",
    "            self.layerfuncs.append(cfuncs)\n",
    "            self.layerweights.append(l.kernel)\n",
    "            \n",
    "        self.saved_logs = {}\n",
    "        input_tensors = [model.inputs[0],\n",
    "                         model.sample_weights[0],\n",
    "                         model.targets[0],\n",
    "                         K.learning_phase(),\n",
    "        ]\n",
    "        self.get_gradients = K.function(inputs=input_tensors, outputs=model.optimizer.get_gradients(model.total_loss, self.layerweights))\n",
    "            \n",
    "        self.get_loss = K.function(inputs=input_tensors, outputs=model.total_loss)\n",
    "\n",
    "            \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if not self.whentodo(epoch):\n",
    "            self._log_gradients = False\n",
    "        else:\n",
    "            self._log_gradients = True\n",
    "            self._batch_todo_ixs = {}\n",
    "            self._batch_gradients = {}\n",
    "            self._batch_weightnorm = []\n",
    "            for cdata, cdataname in ((trn,'trn'), (tst, 'tst')):\n",
    "                self._batch_gradients[cdataname] = [ [] for _ in PLOT_LAYERS ]\n",
    "                ixs = list(range(len(cdata.X)))\n",
    "                np.random.shuffle(ixs)\n",
    "                self._batch_todo_ixs[cdataname] = ixs\n",
    "\n",
    "            \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        if not self._log_gradients:\n",
    "            return\n",
    "        \n",
    "        for cdata, cdataname, istrain in ((trn,'trn', 1), (tst, 'tst', 0)):\n",
    "            cur_ixs = self._batch_todo_ixs[cdataname][:SGD_BATCHSIZE]\n",
    "            if len(cur_ixs) < SGD_BATCHSIZE:\n",
    "                continue\n",
    "            inputs = [cdata.X[cur_ixs,:], [1,]*len(cur_ixs), cdata.Y[cur_ixs,:], istrain]\n",
    "            for lndx, g in enumerate(self.get_gradients(inputs)):\n",
    "                oneDgrad = np.reshape(g, -1, 1)\n",
    "                self._batch_gradients[cdataname][lndx].append(oneDgrad)\n",
    "                \n",
    "            #for layerid in PLOT_LAYERS:\n",
    "            #    w = K.get_value(self.model.layers[layerid].kernel)\n",
    "            #    self._batch_weightnorm.append(np.linalg.norm(w))\n",
    "                \n",
    "            # Advance the indexing\n",
    "            self._batch_todo_ixs[cdataname] = self._batch_todo_ixs[cdataname][SGD_BATCHSIZE:]\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if not self.whentodo(epoch):\n",
    "            return\n",
    "        \n",
    "        model = self.model\n",
    "        self._log_gradients = True\n",
    "        \n",
    "        l = OrderedDict()\n",
    "        \n",
    "        # Get overall performance\n",
    "        for cdata, cdataname, istrain in ((trn,'trn',1), (tst, 'tst',0)):\n",
    "            l['%s_loss'%cdataname] = self.get_loss([cdata.X, [1,]*len(cdata.X), cdata.Y, istrain]).flat[0]\n",
    "            \n",
    "        # Based on https://github.com/ravidziv/IDNNs/blob/1c4926f641d4306af7ae37325358be19e8f4d276/idnns/plots/plot_gradients.py\n",
    "        for cdata, cdataname in ((trn,'trn'), (tst, 'tst')):\n",
    "            for lndx, layerid in enumerate(PLOT_LAYERS):\n",
    "                weights_norm = np.linalg.norm(K.get_value(model.layers[layerid].kernel))\n",
    "                stackedgrads = np.stack(self._batch_gradients[cdataname][lndx], axis=1)\n",
    "                #print(cdataname, lndx, stackedgrads.shape)\n",
    "                gradmean = np.linalg.norm(stackedgrads.mean(axis=1))\n",
    "                gradstd  = np.linalg.norm(stackedgrads.std(axis=1))\n",
    "                l['%s_layer_%d_weightsnorm' % (cdataname, lndx)] = weights_norm\n",
    "                l['%s_layer_%d_gradmean' % (cdataname, lndx)] = gradmean\n",
    "                l['%s_layer_%d_gradstd' % (cdataname, lndx)]  = gradstd\n",
    "                        \n",
    "        for lndx, cfuncs in enumerate(self.layerfuncs):\n",
    "            # Double check\n",
    "            trndata = trn.X[::20]\n",
    "            tstdata = tst.X[::10]\n",
    "            r = scipy.optimize.minimize_scalar(wrapKfunc(cfuncs['loo'], trndata), method='brent')\n",
    "            l['trn_layer_%d_h_loo'%lndx] = r.fun\n",
    "            l['trn_layer_%d_logvar'%lndx] = r.x\n",
    "            l['trn_layer_%d_h_upper'%lndx] = wrapKfunc(cfuncs['upper'], trndata)(r.x)\n",
    "            l['trn_layer_%d_h_lower'%lndx] = wrapKfunc(cfuncs['lower'], trndata)(r.x)\n",
    "\n",
    "            r = scipy.optimize.minimize_scalar(wrapKfunc(cfuncs['loo'], tstdata), method='brent')\n",
    "            l['tst_layer_%d_h_loo' %lndx] = r.fun\n",
    "            l['tst_layer_%d_logvar'%lndx] = r.x\n",
    "            l['tst_layer_%d_h_upper'%lndx] = wrapKfunc(cfuncs['upper'], tstdata)(r.x)\n",
    "            l['tst_layer_%d_h_lower'%lndx] = wrapKfunc(cfuncs['lower'], tstdata)(r.x)\n",
    "            \n",
    "            trndata = trn.X[::10]\n",
    "            tstdata = tst.X\n",
    "            trnlayeroutput = cfuncs['output']([trndata, 0])[0]\n",
    "            tstlayeroutput = cfuncs['output']([tstdata, 0])[0]\n",
    "            l['trn_layer_%d_h_bin'%lndx] = simplebinmi.bin_calc_information(trndata, trnlayeroutput, num_of_bins=NUMBER_OF_BINS)\n",
    "            l['tst_layer_%d_h_bin'%lndx] = simplebinmi.bin_calc_information(tstdata, tstlayeroutput, num_of_bins=NUMBER_OF_BINS)\n",
    "            l['trn_layer_%d_h_binstd'%lndx] = simplebinmi.bin_calc_information(trndata,0.5* scipy.stats.zscore(trnlayeroutput), num_of_bins=NUMBER_OF_BINS)\n",
    "            l['tst_layer_%d_h_binstd'%lndx] = simplebinmi.bin_calc_information(tstdata,0.5* scipy.stats.zscore(tstlayeroutput), num_of_bins=NUMBER_OF_BINS)\n",
    "\n",
    "        for k,v in l.items():\n",
    "            print(k,\"=\",v)\n",
    "            logs[k] = v\n",
    "\n",
    "        self.saved_logs[epoch] = l.copy()\n",
    "        \n",
    "            \n",
    "input_layer  = keras.layers.Input((trn.X.shape[1],))\n",
    "hidden_output = keras.layers.Dense(1024, activation=ACTIVATION)(input_layer)\n",
    "hidden_output = keras.layers.Dense(5  , activation=ACTIVATION)(hidden_output)\n",
    "hidden_output = keras.layers.Dense(5  , activation=ACTIVATION)(hidden_output)\n",
    "\n",
    "outputs  = keras.layers.Dense(trn.nb_classes, activation='softmax')(hidden_output)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=outputs)\n",
    "optimizer = keras.optimizers.SGD(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "def do_report(epoch):\n",
    "    if epoch < 20:\n",
    "        return True\n",
    "    elif epoch < 100:\n",
    "        return (epoch % 2 == 0)\n",
    "    else:\n",
    "        return (epoch % 20 == 0)\n",
    "    \n",
    "reporter   = Reporter(whentodo=do_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r = model.fit(x=trn.X, y=trn.Y, verbose=2, batch_size=SGD_BATCHSIZE, epochs=NUM_EPOCHS, \n",
    "              validation_data=(tst.X, tst.Y), callbacks=[reporter,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = sorted(reporter.saved_logs.keys())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5*len(PLOT_LAYERS)))\n",
    "import matplotlib.gridspec as gridspec\n",
    "gs = gridspec.GridSpec(len(PLOT_LAYERS)+1, 2, )\n",
    "\n",
    "for colndx, t in enumerate(['trn','tst']):\n",
    "    plt.subplot(gs[0,colndx])\n",
    "    plt.plot(epochs, [reporter.saved_logs[epoch][t+'_loss'] for epoch in epochs])\n",
    "    plt.ylim([0, plt.ylim()[1]])\n",
    "    plt.ylabel('Cross entropy loss')\n",
    "    \n",
    "for rowndx, lndx in enumerate(PLOT_LAYERS):\n",
    "    for colndx, t in enumerate(['trn','tst']):\n",
    "        plt.subplot(gs[1+rowndx,colndx])\n",
    "        epochs = sorted(reporter.saved_logs.keys())\n",
    "\n",
    "        plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_h_upper' % rowndx] for epoch in epochs], 'r:', label=\"$H_{KL}$\")\n",
    "        plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_h_lower' % rowndx] for epoch in epochs], 'g:', label=\"$H_{BD}$\")\n",
    "        \n",
    "        # plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_h_loo'   % rowndx] for epoch in epochs], 'r', label=\"$H_{loo}$\")\n",
    "        plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_h_bin'   % rowndx] for epoch in epochs], 'b', label=\"$H_{bin}$\")\n",
    "        plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_h_binstd'% rowndx] for epoch in epochs], 'k', label=\"$H_{binstd}$\")\n",
    "\n",
    "        plt.title('%s layer %d'%(t,lndx))\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Layer Activity Entropy')\n",
    "        \n",
    "        if rowndx == 1 and t == 'tst':\n",
    "            plt.legend(loc='lower right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "gs = gridspec.GridSpec(1,len(PLOT_LAYERS))\n",
    "epochs = sorted(reporter.saved_logs.keys())\n",
    "for lndx, layerid in enumerate(PLOT_LAYERS):\n",
    "    plt.subplot(gs[0,lndx])\n",
    "    means = np.array([reporter.saved_logs[epoch]['trn_layer_%d_gradmean' % lndx] for epoch in epochs])\n",
    "    stds  = np.array([reporter.saved_logs[epoch]['trn_layer_%d_gradstd' % lndx] for epoch in epochs])\n",
    "    plt.plot(epochs, means, 'b', label=\"Mean\")\n",
    "    plt.plot(epochs, stds, 'orange', label=\"Std\")\n",
    "    plt.plot(epochs, means/stds, 'red', label=\"SNR\")\n",
    "    plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_weightsnorm' % lndx] for epoch in epochs], 'g', label=\"||W||\")\n",
    "    \n",
    "    #plt.ylabel('Layer %d - SNR'%rowndx)\n",
    "    #plt.subplot(gs[rowndx*2+1,colndx])\n",
    "    #plt.plot(epochs, [reporter.saved_logs[epoch][t+'_layer_%d_weightsnorm' % rowndx] for epoch in epochs], 'b', label=\"Weight norm\")\n",
    "    #plt.ylabel('Layer %d - ||Weights||'%rowndx)\n",
    "    #plt.xlabel('Epochs')\n",
    "    plt.title('Layer %d'%layerid)\n",
    "    plt.gca().set_xscale(\"log\", nonposx='clip')\n",
    "    plt.gca().set_yscale(\"log\", nonposy='clip')\n",
    "    \n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('run_output_%s.pdf'%ACTIVATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "asdadsf\n",
    "epochs = sorted(reporter.saved_logs.keys())\n",
    "cm = plt.cm.get_cmap('inferno')\n",
    "\n",
    "for colndx, t in enumerate(['trn','tst']):\n",
    "    loss = [reporter.saved_logs[epoch][t+'_loss'] for epoch in epochs]\n",
    "    plt.figure()\n",
    "    for lndx, layerid in enumerate(PLOT_LAYERS):\n",
    "        upperh = [reporter.saved_logs[epoch][t+'_layer_%d_h_upper' % lndx] for epoch in epochs]\n",
    "        lowerh = [reporter.saved_logs[epoch][t+'_layer_%d_h_lower' % lndx] for epoch in epochs]\n",
    "        sc=plt.scatter(loss, upperh, c=epochs, cmap=cm, edgecolor='none', label=\"$H_{KL}$\")\n",
    "        plt.scatter(loss, lowerh, c=epochs, cmap=cm, edgecolor='none', label=\"$H_{BD}$\")\n",
    "    plt.colorbar(sc, label='Epoch')\n",
    "    plt.xlabel('Cross-entropy loss')\n",
    "    plt.ylabel('H(hidden layer))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sortedk = sorted(reporter.saved_batch_logs.keys())\n",
    "meangrads = np.array([reporter.saved_batch_logs[k][0] for k in sortedk])\n",
    "stdgrads  = np.array([reporter.saved_batch_logs[k][1] for k in sortedk])\n",
    "\n",
    "plt.plot(sortedk, meangrads, label='m')\n",
    "plt.hold('on')\n",
    "plt.plot(sortedk, stdgrads, label='std')\n",
    "\n",
    "\n",
    "asdfassdfsfd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
